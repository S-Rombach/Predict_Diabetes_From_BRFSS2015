{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training of a Logistic Regression model with best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "from src.config import (\n",
    "    DATA_SPLIT_DIR,\n",
    "    TRAIN_RAW_FILENAME,\n",
    "    VALIDATION_RAW_FILENAME,\n",
    "    MODELS_DIR,\n",
    "    STUDY_DIR,\n",
    "    MODEL_ALIASES\n",
    ")\n",
    "from src.model_evaluation import evaluate_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_orders = {\n",
    "    \"GenHlth\": [\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"],\n",
    "    \"Age\": [\n",
    "        \"18-24\",\n",
    "        \"25-29\",\n",
    "        \"30-34\",\n",
    "        \"35-39\",\n",
    "        \"40-44\",\n",
    "        \"45-49\",\n",
    "        \"50-54\",\n",
    "        \"55-59\",\n",
    "        \"60-64\",\n",
    "        \"65-69\",\n",
    "        \"70-74\",\n",
    "        \"75-79\",\n",
    "        \"80+\",\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        \"no school\",\n",
    "        \"elementary\",\n",
    "        \"some high school\",\n",
    "        \"high school graduate\",\n",
    "        \"college\",\n",
    "        \"college graduate\",\n",
    "    ],\n",
    "    \"Income\": [\"<$10k\", \"<$15k\", \"<$20k\", \"<$25k\", \"<$35k\", \"<$50k\", \"<$75k\", \">$75k\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from src.transformers import MissingFlagTransformer, CategoryFromThresholdTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "nominal_cols = [\n",
    "    \"HighBP\",\n",
    "    \"HighChol\",\n",
    "    \"CholCheck\",\n",
    "    \"Smoker\",\n",
    "    \"Stroke\",\n",
    "    \"HeartDiseaseorAttack\",\n",
    "    \"PhysActivity\",\n",
    "    \"Fruits\",\n",
    "    \"Veggies\",\n",
    "    \"HvyAlcoholConsump\",\n",
    "    \"AnyHealthcare\",\n",
    "    \"NoDocbcCost\",\n",
    "    \"DiffWalk\",\n",
    "    \"Sex\",\n",
    "]\n",
    "nominal_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"ohe\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse_output=False),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ordinal_cols = [\"GenHlth\", \"Age\", \"Education\", \"Income\"]\n",
    "\n",
    "ordinal_categories = [ordinal_orders[col] for col in ordinal_cols]\n",
    "\n",
    "ordinal_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encode\", OrdinalEncoder(categories=ordinal_categories)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "missing_val_cols = ordinal_cols + nominal_cols\n",
    "missing_val_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"flags\",\n",
    "            MissingFlagTransformer(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "cat_gens = {\"BMI\": [20, 30, 40, 50, 60], \"MentHlth\": [0, 5], \"PhysHlth\": [0, 5]}\n",
    "cat_gen_cols = [\"BMI\", \"MentHlth\", \"PhysHlth\"]\n",
    "cat_gen_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\n",
    "            \"cat_gen\",\n",
    "            CategoryFromThresholdTransformer([cat_gens[c] for c in cat_gen_cols]),\n",
    "        ),\n",
    "        (\n",
    "            \"enc\",\n",
    "            OrdinalEncoder(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ord\", ordinal_pipe, ordinal_cols),\n",
    "        (\"nom\", nominal_pipe, nominal_cols),\n",
    "        (\"miss\", missing_val_pipe, missing_val_cols),\n",
    "        (\"cg\", cat_gen_pipe, cat_gen_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=6.191485524607749,\n",
    "    fit_intercept=True,\n",
    "    random_state=0,\n",
    "    max_iter=100000,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"sample\",\n",
    "            RandomOverSampler(\n",
    "                random_state=5,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"clf\",\n",
    "            classifier,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Load and transform train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "df_train_raw = pd.read_csv(os.path.join(DATA_SPLIT_DIR, TRAIN_RAW_FILENAME))\n",
    "features_train_raw = df_train_raw.drop(\"Diabetes_012\", axis=1)\n",
    "target_train_raw = df_train_raw[\"Diabetes_012\"].replace({\"pre\": \"dia\"})\n",
    "\n",
    "\n",
    "df_val_raw = pd.read_csv(os.path.join(DATA_SPLIT_DIR, VALIDATION_RAW_FILENAME))\n",
    "features_val_raw = df_val_raw.drop(\"Diabetes_012\", axis=1)\n",
    "target_val_raw = df_val_raw[\"Diabetes_012\"].replace({\"pre\": \"dia\"})\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "target_train_enc = labelencoder.fit_transform(target_train_raw)\n",
    "target_val_enc = labelencoder.transform(target_val_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_purpose = \",\".join(labelencoder.classes_)\n",
    "clf_alias = MODEL_ALIASES[classifier.__class__.__name__]\n",
    "special_features = \"rndover,missingflags,catgen\"\n",
    "\n",
    "start_timestamp = datetime.now()\n",
    "\n",
    "pipeline.fit(features_train_raw, target_train_enc)\n",
    "\n",
    "train_end_timestamp = datetime.now()\n",
    "training_duration = train_end_timestamp - start_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Evaluation on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_val_pred = pipeline.predict(features_val_raw)\n",
    "\n",
    "target_val_pred_proba = None\n",
    "\n",
    "if hasattr(classifier, \"predict_proba\"):\n",
    "    target_val_pred_proba = pipeline.predict_proba(features_val_raw)\n",
    "\n",
    "    if target_train_raw.nunique() <= 2:\n",
    "        target_val_pred_proba = target_val_pred_proba[:, 1]\n",
    "\n",
    "results = evaluate_classifier(\n",
    "    classifier=classifier,\n",
    "    labels=list(labelencoder.classes_),\n",
    "    target_truth=target_val_raw,\n",
    "    target_pred=labelencoder.inverse_transform(target_val_pred),\n",
    "    target_pred_proba=target_val_pred_proba,\n",
    "    timestamp=train_end_timestamp,\n",
    "    model_purpose=model_purpose,\n",
    "    special_features=special_features,\n",
    ")\n",
    "results[\"training_duration\"] = training_duration.seconds\n",
    "\n",
    "print(\n",
    "    f\"training duration {training_duration.days} d {(training_duration.seconds // 3600)} h\"\n",
    "    f\" {(training_duration.seconds % 3600) // 60} m {training_duration.seconds % 60} s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_evaluation import (\n",
    "    get_confusion_matrix_from_results_as_df,\n",
    "    get_classification_report_from_results_as_df,\n",
    ")\n",
    "print(\"precision:\", results[\"precision\"])\n",
    "print(\"F1:\", results[\"f1\"])\n",
    "print(\"bal acc:\", results[\"bal_accuracy\"])\n",
    "print(\"roc auc:\", results[\"roc_auc_score\"])\n",
    "\n",
    "print()\n",
    "print(\"confusion_matrix\")\n",
    "display(get_confusion_matrix_from_results_as_df(results))\n",
    "\n",
    "print()\n",
    "print(\"classification_report\")\n",
    "display(get_classification_report_from_results_as_df(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The model achieved an overall macro f1 score of 0.64 and macro recall of 0.75. The f1 score is lessened by the low precision of 0.32 of the positive class, indicating that while the model is good at identifying true positives, it also has a high rate of false positives. The macro recall of 0.75 suggests that the model is able to capture a significant portion of the actual positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Save model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = results[\"model_name\"]\n",
    "\n",
    "folder = os.path.join(MODELS_DIR, model_name)\n",
    "filename = os.path.join(folder, model_name)\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "with open(f\"{filename}.model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "with open(f\"{filename}.pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "with open(f\"{filename}.label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(labelencoder, f)\n",
    "\n",
    "with open(f\"{filename}.model.txt\", \"w\") as f:\n",
    "    f.write(str(classifier))\n",
    "\n",
    "with open(f\"{filename}.results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "with open(f\"{filename}.pipeline_params.txt\", \"w\") as f:\n",
    "    f.write(preprocessor.get_params().__str__())\n",
    "\n",
    "with open(f\"{filename}.model_params.json\", \"w\") as f:\n",
    "    json.dump(classifier.get_params(), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "feature_names = pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "for r, rw in [\n",
    "    (\"num__\", \"\"),\n",
    "    (\"cg__\", \"\"),\n",
    "    (\"ord__\", \"\"),\n",
    "    (\"nom__\", \"\"),\n",
    "    (\"miss__\", \"\"),\n",
    "    (\"_y\", \"\"),\n",
    "    (\"_isna\", \" missing\"),\n",
    "]:\n",
    "    feature_names = [x.replace(r, rw) for x in feature_names]\n",
    "feature_importances = pd.DataFrame(index=feature_names)\n",
    "feature_importances[\"importance\"] = np.abs(classifier.coef_[0])\n",
    "feature_importances.sort_values(by=\"importance\", ascending=False, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 0.3 * len(feature_importances)))\n",
    "sns.barplot(x=feature_importances[\"importance\"], y=feature_importances.index, ax=ax)\n",
    "ax.set_title(f\"feature importances for model '{model_name}'\")\n",
    "for i, (val, label) in enumerate(\n",
    "    zip(feature_importances[\"importance\"], feature_importances.index)\n",
    "):\n",
    "    ax.text(val, i, f\" {val:.4f}\", va=\"center\", ha=\"left\", fontsize=8)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The feature with the highest importance in the Logistic Regression model is `Cholesterol Check`, indicating whether the patient underwent a cholesterol check within the last five years. The next most relevant features, though with a lower contribution, are `GenHlth_isna` (missing information on general health) and `HighBP_y` (presence of high blood pressure). `BMI_cat` ranks further down in importance, which is notable given that BMI is commonly regarded as a major risk factor for diabetes. Similarly, `Sex` and `Age_cat` show relatively low importance in this model, despite being widely recognized as key variables in diabetes risk assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict-diabetes-from-brfss2015",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
