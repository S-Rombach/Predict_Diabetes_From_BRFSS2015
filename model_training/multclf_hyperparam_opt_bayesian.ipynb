{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter optimization `no dia` vs (`pre`, `dia`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_orders = {\n",
    "    \"GenHlth\": [\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"],\n",
    "    \"Age\": [\n",
    "        \"18-24\",\n",
    "        \"25-29\",\n",
    "        \"30-34\",\n",
    "        \"35-39\",\n",
    "        \"40-44\",\n",
    "        \"45-49\",\n",
    "        \"50-54\",\n",
    "        \"55-59\",\n",
    "        \"60-64\",\n",
    "        \"65-69\",\n",
    "        \"70-74\",\n",
    "        \"75-79\",\n",
    "        \"80+\",\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        \"no school\",\n",
    "        \"elementary\",\n",
    "        \"some high school\",\n",
    "        \"high school graduate\",\n",
    "        \"college\",\n",
    "        \"college graduate\",\n",
    "    ],\n",
    "    \"Income\": [\"<$10k\", \"<$15k\", \"<$20k\", \"<$25k\", \"<$35k\", \"<$50k\", \"<$75k\", \">$75k\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "from src.config import (\n",
    "    DATA_SPLIT_DIR,\n",
    "    TRAIN_RAW_FILENAME,\n",
    "    VALIDATION_RAW_FILENAME,\n",
    "    MODELS_DIR,\n",
    "    STUDY_DIR,\n",
    "    MODEL_ALIASES\n",
    ")\n",
    "from src.model_evaluation import evaluate_classifier\n",
    "\n",
    "os.makedirs(STUDY_DIR, exist_ok=True)\n",
    "\n",
    "# For Bayesian Optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from src.transformers import MissingFlagTransformer, CategoryFromThresholdTransformer\n",
    "\n",
    "nominal_cols = [\n",
    "    \"HighBP\",\n",
    "    \"HighChol\",\n",
    "    \"CholCheck\",\n",
    "    \"Smoker\",\n",
    "    \"Stroke\",\n",
    "    \"HeartDiseaseorAttack\",\n",
    "    \"PhysActivity\",\n",
    "    \"Fruits\",\n",
    "    \"Veggies\",\n",
    "    \"HvyAlcoholConsump\",\n",
    "    \"AnyHealthcare\",\n",
    "    \"NoDocbcCost\",\n",
    "    \"DiffWalk\",\n",
    "    \"Sex\",\n",
    "]\n",
    "nominal_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"ohe\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse_output=False),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ordinal_cols = [\"GenHlth\", \"Age\", \"Education\", \"Income\"]\n",
    "\n",
    "ordinal_categories = [ordinal_orders[col] for col in ordinal_cols]\n",
    "\n",
    "ordinal_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encode\", OrdinalEncoder(categories=ordinal_categories)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "numeric_cols = []  # [\"MentHlth\", \"PhysHlth\"]  # \"BMI\",\n",
    "num_pipe = Pipeline(\n",
    "    [(\"impute\", SimpleImputer(strategy=\"median\")), (\"scale\", StandardScaler())]\n",
    ")\n",
    "\n",
    "missing_val_cols = numeric_cols + ordinal_cols + nominal_cols\n",
    "missing_val_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"flags\",\n",
    "            MissingFlagTransformer(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "cat_gens = {\"BMI\": [20, 25, 30], \"MentHlth\": [0, 5, 10], \"PhysHlth\": [0, 5, 10]}\n",
    "cat_gen_cols = [\"BMI\", \"MentHlth\", \"PhysHlth\"]\n",
    "cat_gen_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\n",
    "            \"cat_gen\",\n",
    "            CategoryFromThresholdTransformer([cat_gens[c] for c in cat_gen_cols]),\n",
    "        ),\n",
    "        (\n",
    "            \"enc\",\n",
    "            OrdinalEncoder(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, numeric_cols),\n",
    "        (\"ord\", ordinal_pipe, ordinal_cols),\n",
    "        (\"nom\", nominal_pipe, nominal_cols),\n",
    "        (\"miss\", missing_val_pipe, missing_val_cols),\n",
    "        (\"cg\", cat_gen_pipe, cat_gen_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "classifier = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"sample\",\n",
    "            RandomOverSampler(\n",
    "                random_state=5,\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Load and transform train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "df_train_raw = pd.read_csv(os.path.join(DATA_SPLIT_DIR, TRAIN_RAW_FILENAME))\n",
    "features_train_raw = df_train_raw.drop(\"Diabetes_012\", axis=1)\n",
    "target_train_raw = df_train_raw[\"Diabetes_012\"].replace({\"pre\": \"dia\"})\n",
    "\n",
    "\n",
    "df_val_raw = pd.read_csv(os.path.join(DATA_SPLIT_DIR, VALIDATION_RAW_FILENAME))\n",
    "features_val_raw = df_val_raw.drop(\"Diabetes_012\", axis=1)\n",
    "target_val_raw = df_val_raw[\"Diabetes_012\"].replace({\"pre\": \"dia\"})\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "target_train_enc = labelencoder.fit_transform(target_train_raw)\n",
    "target_val_enc = labelencoder.transform(target_val_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Define Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = [\n",
    "    # GaussianNB,\n",
    "    LogisticRegression,\n",
    "    # RidgeClassifier,\n",
    "    # GradientBoostingClassifier,\n",
    "    # HistGradientBoostingClassifier,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_gen_thresholds = [\n",
    "    (\n",
    "        \"cg_bmi_th\",\n",
    "        [\n",
    "            [30],\n",
    "            [20, 30],\n",
    "            [30, 45],\n",
    "            [20, 30, 40],\n",
    "            [20, 30, 40, 50],\n",
    "            [20, 30, 40, 50, 60],\n",
    "        ],\n",
    "    ),\n",
    "    (\"cg_mh_th\", [[0], [10], [0, 5], [0, 10, 20], [10, 20]]),\n",
    "    (\"cg_ph_th\", [[0], [10], [0, 5], [0, 10, 20], [10, 20]]),\n",
    "]\n",
    "\n",
    "cat_gen_thresholds = {\n",
    "    name: {\n",
    "        f\"[{','.join(sorted([str(x) for x in lst]))}]\": sorted(lst)\n",
    "        for lst in thresholds\n",
    "    }\n",
    "    for name, thresholds in cat_gen_thresholds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_objective(model_class):\n",
    "\n",
    "\n",
    "    def suggest_params(trial, model_name: str) -> dict:\n",
    "\n",
    "\n",
    "        if model_name == \"GaussianNB\":\n",
    "            return {\n",
    "                \"var_smoothing\": trial.suggest_float(\"var_smoothing\", 1e-11, 1e-6, log=True)\n",
    "            }\n",
    "\n",
    "        elif model_name == \"LogisticRegression\":\n",
    "            penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "            return {\n",
    "                \"C\": trial.suggest_float(\"C\", 1e-4, 1e2, log=True),\n",
    "                \"penalty\": penalty,\n",
    "                \"solver\": \"lbfgs\" if penalty != \"none\" else \"saga\",\n",
    "                \"max_iter\": 1000,\n",
    "                \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "                \"random_state\": 0,\n",
    "            }\n",
    "\n",
    "        elif model_name == \"RidgeClassifier\":\n",
    "            return {\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 100.0, log=True),\n",
    "                \"solver\": trial.suggest_categorical(\n",
    "                    \"solver\", [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sag\"]\n",
    "                ),\n",
    "                \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "                \"tol\": trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True),\n",
    "                \"random_state\": 0,\n",
    "            }\n",
    "\n",
    "        elif model_name == \"GradientBoostingClassifier\":\n",
    "            return {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "                \"max_features\": trial.suggest_categorical(\n",
    "                    \"max_features\", [\"sqrt\", \"log2\", None]\n",
    "                ),\n",
    "                \"random_state\": 0,\n",
    "            }\n",
    "\n",
    "        elif model_name == \"HistGradientBoostingClassifier\":\n",
    "            return {\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "                \"max_iter\": trial.suggest_int(\"max_iter\", 100, 300),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "                \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 1e-4, 10.0, log=True),\n",
    "                \"early_stopping\": True,\n",
    "                \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 15, 100),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 10, 50),\n",
    "                \"random_state\": 0,\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"return maximized f1-score\"\"\"\n",
    "\n",
    "        # search space\n",
    "        pipe_params = {\n",
    "            cg_name: trial.suggest_categorical(cg_name, choices=list(thresholds.keys()))\n",
    "            for cg_name, thresholds in cat_gen_thresholds.items()\n",
    "        }\n",
    "        pipeline.set_params(\n",
    "            preprocessor__cg__cat_gen__thresholds=[\n",
    "                cat_gen_thresholds[cg][cat] for cg, cat in pipe_params.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        params = suggest_params(trial, model_class.__name__)\n",
    "\n",
    "        # random forest classifier object\n",
    "        model = model_class(**params)\n",
    "\n",
    "\n",
    "        pipeline.named_steps[\"clf\"] = model\n",
    "\n",
    "        # initiating cv\n",
    "        scores = cross_val_score(\n",
    "            estimator=pipeline,\n",
    "            X=features_train_raw,\n",
    "            y=target_train_enc,\n",
    "            scoring=\"f1_macro\",\n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        scores = [score for score in scores if not np.isnan(score)]\n",
    "\n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Run study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier_cls in classifiers:\n",
    "    print()\n",
    "    print(\"#####################################\")\n",
    "    print(classifier_cls.__name__)\n",
    "    print()\n",
    "    model_purpose = \",\".join(labelencoder.classes_)\n",
    "    clf_alias = (\n",
    "        MODEL_ALIASES[classifier_cls.__name__]\n",
    "        if classifier_cls in MODEL_ALIASES\n",
    "        else classifier_cls.__name__\n",
    "    )\n",
    "    special_features = \"smote\"\n",
    "    study_purpose =\"LogisticRegression_smote_dia,no dia\"# f\"{clf_alias}_{special_features}_{model_purpose}\"\n",
    "\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    study_timestamp_str = \"20250721160037\"\n",
    "    study_name = f\"{study_timestamp_str}_{study_purpose}\"\n",
    "\n",
    "\n",
    "    # create a study (aim to maximize score) und setting a seed (random_state) for reproduceability\n",
    "\n",
    "    # study = optuna.create_study(\n",
    "    #     sampler=TPESampler(seed=42),\n",
    "    #     direction=\"maximize\",\n",
    "    #     study_name=study_name,\n",
    "    #     storage=f\"sqlite:///{os.path.join(STUDY_DIR, study_name)}.db\",\n",
    "    # )\n",
    "\n",
    "\n",
    "    # perform hyperparamter tuning (while timing the process)\n",
    "    time_start = time.time()\n",
    "    study = optuna.load_study(\n",
    "        study_name=study_name,\n",
    "        storage=f\"sqlite:///{os.path.join(STUDY_DIR, study_name)}.db\",\n",
    "    )\n",
    "    obj = get_objective(classifier_cls)\n",
    "    study.optimize(obj, n_trials=N_TRIALS)\n",
    "\n",
    "    start_timestamp = datetime.now()\n",
    "\n",
    "    set_params = study.best_params\n",
    "\n",
    "    if hasattr(classifier_cls, \"random_state\"):\n",
    "        set_params |= {\"random_state\":0}\n",
    "\n",
    "    classifier = classifier_cls( **set_params    )\n",
    "\n",
    "    classifier.fit(features_train_sampled, target_train_sampled_enc)\n",
    "\n",
    "    train_end_timestamp = datetime.now()\n",
    "    training_duration = train_end_timestamp - start_timestamp\n",
    "\n",
    "    ## Prediction\n",
    "    target_val_pred = classifier.predict(features_val_proc)\n",
    "\n",
    "    ## Metrics\n",
    "\n",
    "    target_val_pred_proba = None\n",
    "\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        target_val_pred_proba = classifier.predict_proba(features_val_proc)\n",
    "\n",
    "        if target_train_raw.nunique() <= 2:\n",
    "            target_val_pred_proba = target_val_pred_proba[:, 1]\n",
    "\n",
    "    results = evaluate_classifier(\n",
    "        classifier=classifier,\n",
    "        labels=list(labelencoder.classes_),\n",
    "        target_truth=target_val_raw,\n",
    "        target_pred=labelencoder.inverse_transform(target_val_pred),\n",
    "        target_pred_proba=target_val_pred_proba,\n",
    "        timestamp=train_end_timestamp,\n",
    "        model_purpose=model_purpose,\n",
    "        special_features=special_features,\n",
    "    )\n",
    "    results[\"training_duration\"] = training_duration.seconds\n",
    "\n",
    "    labels = results[\"predicts\"]\n",
    "    model_name = results[\"model_name\"]\n",
    "\n",
    "    ### Save the model and results\n",
    "    folder = os.path.join(MODELS_DIR, model_name)\n",
    "    filename = os.path.join(folder, model_name)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    with open(f\"{filename}.model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(classifier, f)\n",
    "\n",
    "    with open(f\"{filename}.pipeline.pkl\", \"wb\") as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "\n",
    "    with open(f\"{filename}.label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(labelencoder, f)\n",
    "\n",
    "    with open(f\"{filename}.model.txt\", \"w\") as file:\n",
    "        file.write(str(classifier))\n",
    "\n",
    "    with open(f\"{filename}.results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    with open(f\"{filename}.pipeline_params.txt\", \"w\") as f:\n",
    "        f.write(preprocessor.get_params().__str__())\n",
    "\n",
    "    with open(f\"{filename}.model_params.json\", \"w\") as f:\n",
    "        json.dump(classifier.get_params(), f, indent=2)\n",
    "\n",
    "    end_timestamp = datetime.now()\n",
    "    td = end_timestamp - start_timestamp\n",
    "    print(\n",
    "        f\"training duration {td.days} d {(td.seconds // 3600)} h\"\n",
    "        f\" {(td.seconds % 3600) // 60} m {td.seconds % 60} s\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict-diabetes-from-brfss2015",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
